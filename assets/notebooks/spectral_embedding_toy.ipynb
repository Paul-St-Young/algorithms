{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral embedding and clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data $s_1,\\ldots,s_N$\n",
    "\n",
    "To perform our analysis, we construct a toy dataset that is simple to  analyze. This dataset is made of overlapping Gaussian clusters in 2D that are randomly squeezed and rotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib qt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams['font.size'] = 30\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "num_samples = 1000\n",
    "\n",
    "d = 2\n",
    "num_clusters = 4\n",
    "colors = ['r', 'g', 'b', 'k', 'y', 'c', 'm']\n",
    "num_samples_per_cluster = num_samples//num_clusters\n",
    "\n",
    "avg_intercluster_distance = 9.0\n",
    "\n",
    "centers  = np.sqrt(avg_intercluster_distance)*np.random.randn(num_clusters, d)\n",
    "\n",
    "def transformation(point, squeeze_factor, theta):\n",
    "    t = theta\n",
    "    R = np.array([[np.cos(t), -np.sin(t)], [np.sin(t), np.cos(t)]])\n",
    "    squeeze = np.array([[squeeze_factor, 0.0],[0.0, 1.0]])\n",
    "    return np.dot(R, np.dot(squeeze, point))\n",
    "\n",
    "clusters = []\n",
    "data = np.zeros((num_samples, d))\n",
    "for ind_c in range(num_clusters-1):\n",
    "    cluster = []\n",
    "    center = centers[ind_c,:]\n",
    "    covariance = np.eye(d)\n",
    "    \n",
    "    # Squeeze factor\n",
    "    squeeze_factor = np.random.rand()*4\n",
    "    \n",
    "    # Rotation angle\n",
    "    theta = np.random.rand()*2.0*np.pi\n",
    "    \n",
    "    for ind_sc in range(num_samples_per_cluster):\n",
    "        point = np.random.multivariate_normal(center, covariance)\n",
    "        \n",
    "        # Do a non-linear transformation\n",
    "        point = transformation(point, squeeze_factor, theta)\n",
    "        \n",
    "        cluster.append(point)\n",
    "        \n",
    "        ind_sample = ind_c * num_samples_per_cluster + ind_sc\n",
    "        data[ind_sample, :] = point\n",
    "    clusters.append(cluster)\n",
    "    \n",
    "thetas = np.random.rand(num_samples_per_cluster)*2.0*np.pi\n",
    "xs = avg_intercluster_distance * np.cos(thetas)\n",
    "ys = avg_intercluster_distance * np.sin(thetas)\n",
    "cluster = [np.array([xs[ind_t], ys[ind_t]]) for ind_t in range(num_samples_per_cluster)]\n",
    "clusters.append(cluster)\n",
    "data[-num_samples_per_cluster:, :] = np.array(cluster)\n",
    "    \n",
    "# Plot the clusters with different colors.\n",
    "plt.figure()\n",
    "for ind_c in range(num_clusters):\n",
    "    cluster = np.array(clusters[ind_c])\n",
    "    (xs, ys) = zip(cluster.T)\n",
    "    plt.plot(xs, ys, 'o', color=colors[ind_c])\n",
    "    \n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel $K(s_i,s_j)$\n",
    "\n",
    "Spectral embedding and other \"kernel trick\" methods use a kernel $K(s_i, s_j)$ that measures similarity between data points. The choice of kernel is part of the procedure and requires some intuition/trial-and-error.\n",
    "\n",
    "A popular choice of kernel is the Gaussian kernel\n",
    "$$ K(s_i, s_j) = \\exp\\left(- \\frac{||s_i - s_j||^2}{2\\sigma^2}\\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5baf01b810>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% matplotlib qt\n",
    "\n",
    "import numpy.linalg as nla\n",
    "\n",
    "# Gaussian kernel with variance sigma^2.\n",
    "sigma = 0.4\n",
    "def kernel(s_i, s_j):\n",
    "    return np.exp(-nla.norm(s_i - s_j)**2.0 / (2.0*sigma**2.0))\n",
    "\n",
    "# The kernel matrix of the data.\n",
    "kernel_matrix = np.zeros((num_samples, num_samples))\n",
    "for i in range(num_samples):\n",
    "    for j in range(i, num_samples):\n",
    "        kernel_matrix[i,j] = kernel(data[i,:], data[j,:])\n",
    "        kernel_matrix[j,i] = kernel_matrix[i,j]\n",
    "        \n",
    "plt.matshow(kernel_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Laplacian $L$\n",
    "\n",
    "A key step of spectral embedding is the construction of the graph Laplacian, $L = D - K$, where $D_{ij} = \\sum_{l=1}^n K(s_i, s_l) \\delta_{ij}$ is the degree matrix and $K_{ij} = K(s_i, s_j)$ is the edge weight matrix (the kernel matrix). The graph Laplacian can be interpreted as an effective quadratic Hamiltonian for springs coupled Harmonically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5bc4c2f310>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% matplotlib qt\n",
    "\n",
    "degrees = np.sum(kernel_matrix, axis=0)\n",
    "D = np.diag(degrees)\n",
    "K = kernel_matrix\n",
    "\n",
    "L = D - K\n",
    "\n",
    "plt.matshow(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrum of $L$\n",
    "\n",
    "Compute the spectrum of $L$. Ignore the exactly zero eigenvector as it has to always be $(1,\\ldots,1)$, which does not give us useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest eigenvalues = [ -3.15161903e-15   1.36346699e-08   6.10323236e-07   3.99972754e-04\n",
      "   1.77923482e-03   2.29958753e-03   4.23597842e-03   5.73741030e-03]\n"
     ]
    }
   ],
   "source": [
    "% matplotlib qt\n",
    "\n",
    "import numpy.linalg as nla\n",
    "\n",
    "(eigvals, eigvecs) = nla.eigh(L)\n",
    "\n",
    "print('Smallest eigenvalues = {}'.format(eigvals[0:2*num_clusters]))\n",
    "\n",
    "plt.plot(eigvals[1:], 'ro')\n",
    "plt.xlabel('Eigenvalue index')\n",
    "plt.ylabel('Eigenvalue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-means clustering \n",
    "\n",
    "The $k$-means cluster algorithm heuristically groups data into a fixed number, $k$, of clusters by greedily minimizing the intracluster distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's test how well $k$-means can cluster the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms import kmeans\n",
    "\n",
    "# The number of clusters to use in k-means.\n",
    "k = num_clusters\n",
    "# The number of iterations.\n",
    "num_iterations = 20\n",
    "\n",
    "(clusters_info, data_point_assignment, centroids) = kmeans(data, k, num_iterations, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib qt\n",
    "\n",
    "def plot_clusters(clusters_info):\n",
    "    ind_plot = 0\n",
    "    for cluster_inds in clusters_info:\n",
    "        cluster_data = data[cluster_inds, :]\n",
    "\n",
    "        (xs, ys) = zip(cluster_data.T)\n",
    "\n",
    "        plt.plot(xs, ys, 'o', color=colors[ind_plot])\n",
    "\n",
    "        ind_plot += 1\n",
    "\n",
    "    #plt.xlabel('$x$')\n",
    "    #plt.ylabel('$y$')\n",
    "    \n",
    "plot_clusters(clusters_info)\n",
    "for ind_cluster in range(k):\n",
    "    plt.plot([centroids[0,ind_cluster]], [centroids[1,ind_cluster]], '*', markersize=26, color='w', markeredgecolor='k', markeredgewidth=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral embedding\n",
    "\n",
    "Now, instead of doing $k$-means on the original data, we perform $k$-means on the data projected onto the $k$-lowest eigenvalue eigenvectors of the graph Laplacian $L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms import spectral_clustering\n",
    "\n",
    "# The number of clusters to use in k-means.\n",
    "k = num_clusters\n",
    "# The number of iterations.\n",
    "num_iterations = 10\n",
    "# Type of Laplacian to use.\n",
    "L_type = 'unnormalized'\n",
    "(spectral_clusters_info, data_point_assignment, centroids) = spectral_clustering(data, k, num_iterations, kernel, L_type=L_type, verbose=False)\n",
    "\n",
    "L_type = 'symmetric'\n",
    "(spectral_clusters_sym_info, data_point_assignment, centroids) = spectral_clustering(data, k, num_iterations, kernel, L_type=L_type, verbose=False)\n",
    "\n",
    "L_type = 'randomwalk'\n",
    "(spectral_clusters_rw_info, data_point_assignment, centroids) = spectral_clustering(data, k, num_iterations, kernel, L_type=L_type, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib qt\n",
    "\n",
    "plot_clusters(spectral_clusters_info)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit learn implementations\n",
    "\n",
    "Here we test our results against $k$-means and spectral clustering methods implemented in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "\n",
    "# k-means\n",
    "km = KMeans(n_clusters=k).fit(data)\n",
    "\n",
    "skl_kmeans_clusters_info = []\n",
    "for ind_cluster in range(k):\n",
    "    skl_kmeans_clusters_info.append([])\n",
    "\n",
    "for ind_point in range(num_samples):\n",
    "    ind_cluster = km.labels_[ind_point]\n",
    "    skl_kmeans_clusters_info[ind_cluster].append(ind_point)\n",
    "\n",
    "# spectral clustering\n",
    "sc = SpectralClustering(n_clusters=k, affinity='precomputed', assign_labels='kmeans') #, gamma=1.0/sigma**2.0, affinity='rbf').fit(data)\n",
    "sc.fit_predict(kernel_matrix)\n",
    "\n",
    "skl_sc_clusters_info = []\n",
    "for ind_cluster in range(k):\n",
    "    skl_sc_clusters_info.append([])\n",
    "\n",
    "for ind_point in range(num_samples):\n",
    "    ind_cluster = sc.labels_[ind_point]\n",
    "    skl_sc_clusters_info[ind_cluster].append(ind_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side-by-side comparisons\n",
    "\n",
    "Here we show for reference the original clusters, the $k$-means clustering and the spectral clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib qt\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(2,4,1)\n",
    "for ind_c in range(num_clusters):\n",
    "    cluster = np.array(clusters[ind_c])\n",
    "    (xs, ys) = zip(cluster.T)\n",
    "    plt.plot(xs, ys, 'o', color=colors[ind_c])\n",
    "    \n",
    "plt.title('Original clusters')\n",
    "#plt.xlabel('$x$')\n",
    "#plt.ylabel('$y$')\n",
    "\n",
    "titles = ['$k$-means', 'spectral', 'spectral sym', 'spectral rw', 'SKL $k$-means', 'SKL spectral']\n",
    "cluster_datas = [clusters_info, spectral_clusters_info, spectral_clusters_sym_info, spectral_clusters_rw_info, skl_kmeans_clusters_info, skl_sc_clusters_info]\n",
    "\n",
    "ind_subplot = 2\n",
    "for (subplot_title, cluster_data) in zip(titles, cluster_datas):\n",
    "    plt.subplot(2,4,ind_subplot)\n",
    "    \n",
    "    plot_clusters(cluster_data)\n",
    "    \n",
    "    ind_subplot += 1\n",
    "    \n",
    "    plt.title(subplot_title)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
